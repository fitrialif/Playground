{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the use of LSTM for Sentiment Analysis\n",
    "The purpose of this notebook is to assess the accuracy in sentiment analysis of a standard (not fine-tuned) Long-Short Term Memory Recurrent Neural Network (LSTM) and compare it to the NLTK unsupervised built-in tool Sentiment Intensity Analyzer (SIA). To do so, three datasets are used:\n",
    "* UMICH SI650: 7086 comments https://www.kaggle.com/c/si650winter11/data\n",
    "* IMDB Movies Reviews: ~25k reviews https://www.kaggle.com/oumaimahourrane/imdb-reviews\n",
    "* Sentiment140: 1.6M tweets https://www.kaggle.com/kazanova/sentiment140 (for computational reasons, I'm taking a random sample of 10%)\n",
    "\n",
    "All datasets are treated with minimal pre-processing (little word standardization) and splitted into three sets to avoid overfitting: train (60%), test (20%) and validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMICH data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3412</th>\n",
       "      <td>1</td>\n",
       "      <td>Brokeback mountain was beautiful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>1</td>\n",
       "      <td>I love Brokeback Mountain....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5763</th>\n",
       "      <td>0</td>\n",
       "      <td>Not because I hate Harry Potter, but because I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>1</td>\n",
       "      <td>So as felicia's mom is cleaning the table, fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4058</th>\n",
       "      <td>0</td>\n",
       "      <td>The Da Vinci Code sucked big time.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                               text\n",
       "3412          1                Brokeback mountain was beautiful...\n",
       "3424          1                      I love Brokeback Mountain....\n",
       "5763          0  Not because I hate Harry Potter, but because I...\n",
       "1593          1  So as felicia's mom is cleaning the table, fel...\n",
       "4058          0                 The Da Vinci Code sucked big time."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umich = pd.read_csv(\"data/UMICH training.txt\", sep='\\t', names=['sentiment', 'text'], encoding='iso-8859-1')\n",
    "umich['text'] = umich['text'].apply(lambda x: x.lower())\n",
    "umich.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_information(data, text_var, verbose=True):\n",
    "    '''Returns the maximum number of words of a single document and each word frequency'''\n",
    "    maxlen = 0\n",
    "    word_freqs = collections.Counter()\n",
    "\n",
    "    for text in data[text_var]:\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "\n",
    "    if verbose:\n",
    "        print('Max number of words in a single sentence:', maxlen)\n",
    "        print('Number of unique words:', len(word_freqs))\n",
    "    \n",
    "    return maxlen, word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_dicts(max_features, word_freqs):\n",
    "    '''Maps words to indexes'''\n",
    "    vocab_size = min(max_features, len(word_freqs)) + 2\n",
    "    word2index = {x[0]: i+2 for i, x in \n",
    "                    enumerate(word_freqs.most_common(max_features))}\n",
    "    word2index[\"PAD\"] = 0\n",
    "    word2index[\"UNK\"] = 1\n",
    "    index2word = {v:k for k, v in word2index.items()}\n",
    "    return vocab_size, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences2sequences(data, text_var, word2index, max_length):\n",
    "    '''Maps sentences to sequences'''\n",
    "    X = np.empty((data.shape[0], ), dtype=list)\n",
    "\n",
    "    for i, text in enumerate(data[text_var]):\n",
    "        words = nltk.word_tokenize(text.lower())\n",
    "        seqs = []\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                seqs.append(word2index[word])\n",
    "            else:\n",
    "                seqs.append(word2index[\"UNK\"])\n",
    "        X[i] = seqs\n",
    "\n",
    "    # Pad the sequences (left padded with zeros)\n",
    "    return sequence.pad_sequences(X, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test_val(X, y, test_size, val_size=None, random_state=None):\n",
    "    '''Splits data into train, test and validation sets'''\n",
    "    if val_size == None:\n",
    "        val_size = test_size\n",
    "        \n",
    "    test_size = test_size/(1-val_size)\n",
    "    \n",
    "    if random_state == None:\n",
    "        rs2 = None\n",
    "    else:\n",
    "        rs2 = random_state * 45\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=val_size, random_state=random_state)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=test_size, random_state=rs2)\n",
    "    \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_pipeline(data, text_var, score_var, test_size=0.2):\n",
    "    '''Complete pipeline for data to be ingested into LSTM'''\n",
    "    # Get necessary corpus information\n",
    "    maxlen, word_freqs = get_corpus_information(data, text_var)\n",
    "    \n",
    "    # Reduce dimmensionality a bit to avoid overfitting\n",
    "    max_features = int(len(word_freqs) * 0.8)\n",
    "    max_sentence_length = int(maxlen/2)\n",
    "    vocab_size, word2index, index2word = get_mapping_dicts(max_features, word_freqs)\n",
    "    \n",
    "    # convert sentences to sequences\n",
    "    X = sentences2sequences(data, text_var, word2index, max_sentence_length)\n",
    "    \n",
    "    # Split train/test/validation data\n",
    "    X_train, X_test, X_val, y_train, y_test, y_val = split_train_test_val(X, data[score_var], test_size, random_state=845)\n",
    "    print('Train shapes:')\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    print('Test shapes:')\n",
    "    print(X_test.shape, y_test.shape)\n",
    "    print('Validation shapes:')\n",
    "    print(X_val.shape, y_val.shape)\n",
    "    \n",
    "    return {'X_train':X_train, 'X_test':X_test, 'X_val':X_val, \n",
    "            'y_train':y_train, 'y_test':y_test, 'y_val':y_val, \n",
    "            'index2word': index2word, 'vocab_size':vocab_size, \n",
    "            'max_sentence_length':max_sentence_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in a single sentence: 1049\n",
      "Number of unique words: 2327\n",
      "Train shapes:\n",
      "(4150, 524) (4150,)\n",
      "Test shapes:\n",
      "(1384, 524) (1384,)\n",
      "Validation shapes:\n",
      "(1384, 524) (1384,)\n"
     ]
    }
   ],
   "source": [
    "umich_data = preprocessing_pipeline(umich, 'text', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Intensity Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sia_df(data, text_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    df_sia = [sia.polarity_scores(text) for text in data[text_var]]\n",
    "    df_sia = pd.DataFrame(df_sia)\n",
    "    df_sia['sentiment'] = (df_sia['compound'] > 0).astype(int)\n",
    "    df_sia['text'] = data[text_var]\n",
    "    return df_sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>-0.2516</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0</td>\n",
       "      <td>Which is why i said silent hill turned into re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6480</th>\n",
       "      <td>0.2263</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.199</td>\n",
       "      <td>1</td>\n",
       "      <td>, she helped me bobbypin my insanely cool hat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>-0.3612</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>Combining the opinion / review from Gary and G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>-0.4215</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0</td>\n",
       "      <td>This quiz sucks and Harry Potter sucks ok bye..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6436</th>\n",
       "      <td>-0.3182</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok brokeback mountain is such a horrible movie.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      compound    neg    neu    pos  sentiment  \\\n",
       "1819   -0.2516  0.161  0.721  0.118          0   \n",
       "6480    0.2263  0.127  0.674  0.199          1   \n",
       "4537   -0.3612  0.161  0.839  0.000          0   \n",
       "5627   -0.4215  0.379  0.455  0.167          0   \n",
       "6436   -0.3182  0.327  0.467  0.206          0   \n",
       "\n",
       "                                                   text  \n",
       "1819  Which is why i said silent hill turned into re...  \n",
       "6480  , she helped me bobbypin my insanely cool hat ...  \n",
       "4537  Combining the opinion / review from Gary and G...  \n",
       "5627    This quiz sucks and Harry Potter sucks ok bye..  \n",
       "6436    Ok brokeback mountain is such a horrible movie.  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umich_sia = get_sia_df(umich, 'text')\n",
    "umich_sia.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full dataset: 0.8737\n",
      "Accuracy on validation dataset: 0.8822\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on full dataset: {:.4f}'.format(accuracy_score(umich['sentiment'], \n",
    "                                                               umich_sia['sentiment'])))\n",
    "print('Accuracy on validation dataset: {:.4f}'.format(accuracy_score(umich_data['y_val'], \n",
    "        umich_sia['sentiment'].iloc[umich_data['y_val'].index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(data_dict, embedding_size=128, hidden_layer_size=64):\n",
    "    vocab_size = data_dict['vocab_size']\n",
    "    input_length = data_dict['max_sentence_length']\n",
    "\n",
    "    # Build LSTM\n",
    "    lstm = Sequential()\n",
    "    lstm.add(Embedding(vocab_size, embedding_size, input_length=input_length))\n",
    "    lstm.add(SpatialDropout1D(0.2))\n",
    "    lstm.add(LSTM(hidden_layer_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "    lstm.add(Dense(1))\n",
    "    lstm.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    lstm.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    #show the model summary\n",
    "    return lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 524, 128)          238464    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 524, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 287,937\n",
      "Trainable params: 287,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = build_lstm(umich_data, embedding_size=128, hidden_layer_size=64)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4150 samples, validate on 1384 samples\n",
      "Epoch 1/10\n",
      "4150/4150 [==============================] - 24s 6ms/step - loss: 0.2548 - acc: 0.9443 - val_loss: 0.1961 - val_acc: 0.9451\n",
      "Epoch 2/10\n",
      "4150/4150 [==============================] - 25s 6ms/step - loss: 0.1346 - acc: 0.9708 - val_loss: 0.1177 - val_acc: 0.9725\n",
      "Epoch 3/10\n",
      "4150/4150 [==============================] - 23s 6ms/step - loss: 0.0750 - acc: 0.9843 - val_loss: 0.0887 - val_acc: 0.9769\n",
      "Epoch 4/10\n",
      "4150/4150 [==============================] - 24s 6ms/step - loss: 0.0419 - acc: 0.9945 - val_loss: 0.0704 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "4150/4150 [==============================] - 24s 6ms/step - loss: 0.0263 - acc: 0.9966 - val_loss: 0.0622 - val_acc: 0.9812\n",
      "Epoch 6/10\n",
      "4150/4150 [==============================] - 24s 6ms/step - loss: 0.0191 - acc: 0.9969 - val_loss: 0.0618 - val_acc: 0.9827\n",
      "Epoch 7/10\n",
      "4150/4150 [==============================] - 23s 6ms/step - loss: 0.0124 - acc: 0.9990 - val_loss: 0.0669 - val_acc: 0.9798\n",
      "Epoch 8/10\n",
      "4150/4150 [==============================] - 23s 6ms/step - loss: 0.0095 - acc: 0.9988 - val_loss: 0.0652 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "4150/4150 [==============================] - 23s 6ms/step - loss: 0.0062 - acc: 0.9998 - val_loss: 0.0654 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "4150/4150 [==============================] - 24s 6ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0661 - val_acc: 0.9827\n"
     ]
    }
   ],
   "source": [
    "umich_history = lstm.fit(umich_data['X_train'], umich_data['y_train'], batch_size=256, epochs=10,\n",
    "                         validation_data=(umich_data['X_test'], umich_data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Validation Score: 0.9819\n"
     ]
    }
   ],
   "source": [
    "preds = lstm.predict(umich_data['X_val'], batch_size=1024)\n",
    "preds = (preds > 0.5).astype(int)\n",
    "print('LSTM Validation Score: {:.4f}'.format(accuracy_score(umich_data['y_val'], preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear some memory\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM achieves ~10% higher accuracy than SIA on UMICH data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movies Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>first think another Disney movie, might good, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>big fan Stephen King's work, film made even gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  first think another Disney movie, might good, ...          1\n",
       "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
       "2  big fan Stephen King's work, film made even gr...          1\n",
       "3  watched horrid thing TV. Needless say one movi...          0\n",
       "4  truly enjoyed film. acting terrific plot. Jeff...          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = pd.read_csv('data/imdb-reviews.csv', encoding='iso-8859-1')\n",
    "imdb.columns = ['text', 'sentiment']\n",
    "imdb['text'] = imdb['text'].apply(lambda x: x.lower())\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in a single sentence: 1828\n",
      "Number of unique words: 114340\n",
      "Train shapes:\n",
      "(15000, 914) (15000,)\n",
      "Test shapes:\n",
      "(5000, 914) (5000,)\n",
      "Validation shapes:\n",
      "(5000, 914) (5000,)\n"
     ]
    }
   ],
   "source": [
    "imdb_data = preprocessing_pipeline(imdb, 'text', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15651</th>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.219</td>\n",
       "      <td>1</td>\n",
       "      <td>Thief Bagdad treasure. First foremost, good st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13166</th>\n",
       "      <td>0.3840</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.138</td>\n",
       "      <td>1</td>\n",
       "      <td>young Dr. Fanshawe(Mark Letheren), avid archae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.6809</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.144</td>\n",
       "      <td>1</td>\n",
       "      <td>Istanbul another one expatriate films Errol Fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8501</th>\n",
       "      <td>0.7688</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.138</td>\n",
       "      <td>1</td>\n",
       "      <td>Hidden Frontier fan made show, world Star Trek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>0.9781</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1</td>\n",
       "      <td>John Carpenter's Halloween&lt;br /&gt;&lt;br /&gt;Is great...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       compound    neg    neu    pos  sentiment  \\\n",
       "15651    0.9678  0.073  0.708  0.219          1   \n",
       "13166    0.3840  0.144  0.717  0.138          1   \n",
       "2592     0.6809  0.107  0.749  0.144          1   \n",
       "8501     0.7688  0.067  0.795  0.138          1   \n",
       "1595     0.9781  0.132  0.663  0.205          1   \n",
       "\n",
       "                                                    text  \n",
       "15651  Thief Bagdad treasure. First foremost, good st...  \n",
       "13166  young Dr. Fanshawe(Mark Letheren), avid archae...  \n",
       "2592   Istanbul another one expatriate films Errol Fl...  \n",
       "8501   Hidden Frontier fan made show, world Star Trek...  \n",
       "1595   John Carpenter's Halloween<br /><br />Is great...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_sia = get_sia_df(imdb, 'text')\n",
    "imdb_sia.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full dataset: 0.6760\n",
      "Accuracy on validation dataset: 0.6786\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on full dataset: {:.4f}'.format(accuracy_score(imdb['sentiment'], \n",
    "                                                               imdb_sia['sentiment'])))\n",
    "print('Accuracy on validation dataset: {:.4f}'.format(accuracy_score(imdb_data['y_val'], \n",
    "      imdb_sia['sentiment'].iloc[imdb_data['y_val'].index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 914, 128)          11708672  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 914, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 11,758,145\n",
      "Trainable params: 11,758,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = build_lstm(imdb_data, embedding_size=128, hidden_layer_size=64)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "15000/15000 [==============================] - 109s 7ms/step - loss: 0.6668 - acc: 0.6379 - val_loss: 0.5233 - val_acc: 0.7804\n",
      "Epoch 2/10\n",
      "15000/15000 [==============================] - 107s 7ms/step - loss: 0.4247 - acc: 0.8319 - val_loss: 0.3554 - val_acc: 0.8428\n",
      "Epoch 3/10\n",
      "15000/15000 [==============================] - 107s 7ms/step - loss: 0.2611 - acc: 0.9055 - val_loss: 0.3512 - val_acc: 0.8574\n",
      "Epoch 4/10\n",
      "15000/15000 [==============================] - 107s 7ms/step - loss: 0.1771 - acc: 0.9396 - val_loss: 0.3643 - val_acc: 0.8664\n",
      "Epoch 5/10\n",
      "15000/15000 [==============================] - 106s 7ms/step - loss: 0.1249 - acc: 0.9613 - val_loss: 0.3865 - val_acc: 0.8602\n",
      "Epoch 6/10\n",
      "15000/15000 [==============================] - 107s 7ms/step - loss: 0.0840 - acc: 0.9761 - val_loss: 0.4276 - val_acc: 0.8388\n",
      "Epoch 7/10\n",
      "15000/15000 [==============================] - 107s 7ms/step - loss: 0.0595 - acc: 0.9826 - val_loss: 0.4723 - val_acc: 0.8516\n",
      "Epoch 8/10\n",
      "15000/15000 [==============================] - 106s 7ms/step - loss: 0.0497 - acc: 0.9854 - val_loss: 0.5485 - val_acc: 0.8098\n",
      "Epoch 9/10\n",
      "15000/15000 [==============================] - 111s 7ms/step - loss: 0.0389 - acc: 0.9895 - val_loss: 0.5616 - val_acc: 0.8464\n",
      "Epoch 10/10\n",
      "15000/15000 [==============================] - 112s 7ms/step - loss: 0.0302 - acc: 0.9921 - val_loss: 0.5987 - val_acc: 0.8360\n"
     ]
    }
   ],
   "source": [
    "imdb_history = lstm.fit(imdb_data['X_train'], imdb_data['y_train'], batch_size=512, epochs=10,\n",
    "                        validation_data=(imdb_data['X_test'], imdb_data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Validation Score: 0.8360\n"
     ]
    }
   ],
   "source": [
    "preds = lstm.predict(imdb_data['X_val'], batch_size=1024)\n",
    "preds = (preds > 0.5).astype(int)\n",
    "print('LSTM Validation Score: {:.4f}'.format(accuracy_score(imdb_data['y_val'], preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM achieves ~16% higher accurancy than SIA in IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment140 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I almost lost my finger to the ceiling fan.. i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ohh, man.  My favorite SNL surprise of the nig...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Beaker Can't DM @zhenji as he's not following...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Look4acure Yeah!!  make sure you get some of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@AmaNorris wow that last tweet made me seem li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  I almost lost my finger to the ceiling fan.. i...          0\n",
       "1  Ohh, man.  My favorite SNL surprise of the nig...          1\n",
       "2  @Beaker Can't DM @zhenji as he's not following...          0\n",
       "3  @Look4acure Yeah!!  make sure you get some of ...          1\n",
       "4  @AmaNorris wow that last tweet made me seem li...          0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = pd.read_csv('data/sentiment140.csv', header=None, encoding='iso-8859-1').iloc[:,[5, 0]]\n",
    "sent.columns = ['text', 'sentiment']\n",
    "sent = sent.sample(frac=0.1, random_state=452).reset_index(drop=True)\n",
    "sent['sentiment'] = sent['sentiment'].replace(4,1)\n",
    "print(sent.shape)\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove users and links\n",
    "sent['text'] = sent['text'].replace(r'@[^ ]+', '', regex=True)\n",
    "sent['text'] = sent['text'].replace(r'[^ ]+//[^ ]+', '', regex=True)\n",
    "sent['text'] = sent['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words in a single sentence: 115\n",
      "Number of unique words: 88115\n",
      "Train shapes:\n",
      "(96000, 57) (96000,)\n",
      "Test shapes:\n",
      "(32000, 57) (32000,)\n",
      "Validation shapes:\n",
      "(32000, 57) (32000,)\n"
     ]
    }
   ],
   "source": [
    "sent_data = preprocessing_pipeline(sent, 'text', 'sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35255</th>\n",
       "      <td>0.5962</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.328</td>\n",
       "      <td>1</td>\n",
       "      <td>i want to go to jb and demi concert today!!!  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69660</th>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0</td>\n",
       "      <td>oh sadness. and i basically told you what it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152816</th>\n",
       "      <td>0.7450</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.372</td>\n",
       "      <td>1</td>\n",
       "      <td>working a split shift today  but had h**lla fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9118</th>\n",
       "      <td>-0.5106</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>up late with a sick little girl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126392</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>i really need some coffee now!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        compound    neg    neu    pos  sentiment  \\\n",
       "35255     0.5962  0.000  0.672  0.328          1   \n",
       "69660    -0.1027  0.263  0.562  0.175          0   \n",
       "152816    0.7450  0.000  0.628  0.372          1   \n",
       "9118     -0.5106  0.398  0.602  0.000          0   \n",
       "126392    0.0000  0.000  1.000  0.000          0   \n",
       "\n",
       "                                                     text  \n",
       "35255   i want to go to jb and demi concert today!!!  ...  \n",
       "69660    oh sadness. and i basically told you what it ...  \n",
       "152816  working a split shift today  but had h**lla fu...  \n",
       "9118                     up late with a sick little girl   \n",
       "126392                    i really need some coffee now!   "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_sia = get_sia_df(sent, 'text')\n",
    "sent_sia.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on full dataset: 0.6510\n",
      "Accuracy on validation dataset: 0.6479\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy on full dataset: {:.4f}'.format(accuracy_score(sent['sentiment'], \n",
    "                                                               sent_sia['sentiment'])))\n",
    "print('Accuracy on validation dataset: {:.4f}'.format(accuracy_score(sent_data['y_val'], \n",
    "      sent_sia['sentiment'].iloc[sent_data['y_val'].index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 57, 128)           9023232   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 57, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,072,705\n",
      "Trainable params: 9,072,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = build_lstm(sent_data, embedding_size=128, hidden_layer_size=64)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 96000 samples, validate on 32000 samples\n",
      "Epoch 1/10\n",
      "96000/96000 [==============================] - 15s 160us/step - loss: 0.5197 - acc: 0.7511 - val_loss: 0.4800 - val_acc: 0.7768\n",
      "Epoch 2/10\n",
      "96000/96000 [==============================] - 15s 161us/step - loss: 0.4274 - acc: 0.8100 - val_loss: 0.4631 - val_acc: 0.7858\n",
      "Epoch 3/10\n",
      "96000/96000 [==============================] - 15s 158us/step - loss: 0.3810 - acc: 0.8362 - val_loss: 0.4688 - val_acc: 0.7870\n",
      "Epoch 4/10\n",
      "96000/96000 [==============================] - 15s 160us/step - loss: 0.3469 - acc: 0.8538 - val_loss: 0.4797 - val_acc: 0.7848\n",
      "Epoch 5/10\n",
      "96000/96000 [==============================] - 15s 157us/step - loss: 0.3192 - acc: 0.8688 - val_loss: 0.4925 - val_acc: 0.7782\n",
      "Epoch 6/10\n",
      "96000/96000 [==============================] - 15s 157us/step - loss: 0.2988 - acc: 0.8787 - val_loss: 0.5156 - val_acc: 0.7762\n",
      "Epoch 7/10\n",
      "96000/96000 [==============================] - 15s 159us/step - loss: 0.2811 - acc: 0.8868 - val_loss: 0.5359 - val_acc: 0.7722\n",
      "Epoch 8/10\n",
      "96000/96000 [==============================] - 15s 157us/step - loss: 0.2640 - acc: 0.8939 - val_loss: 0.5484 - val_acc: 0.7660\n",
      "Epoch 9/10\n",
      "96000/96000 [==============================] - 15s 157us/step - loss: 0.2490 - acc: 0.8991 - val_loss: 0.5741 - val_acc: 0.7668\n",
      "Epoch 10/10\n",
      "96000/96000 [==============================] - 15s 157us/step - loss: 0.2364 - acc: 0.9044 - val_loss: 0.5851 - val_acc: 0.7630\n"
     ]
    }
   ],
   "source": [
    "sent_history = lstm.fit(sent_data['X_train'], sent_data['y_train'], batch_size=2048, epochs=10,\n",
    "                        validation_data=(sent_data['X_test'], sent_data['y_test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Validation Score: 0.7650\n"
     ]
    }
   ],
   "source": [
    "preds = lstm.predict(sent_data['X_val'], batch_size=1024)\n",
    "preds = (preds > 0.5).astype(int)\n",
    "print('LSTM Validation Score: {:.4f}'.format(accuracy_score(sent_data['y_val'], preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM achieves ~12% higher accuracy than SIA in Sentiment140 dataset.\n",
    "\n",
    "In summary, a simple not especifically tuned and not widely trained LSTM seems to consistently outperform the standard SIA implementation. Nevertheless, the ease and speed of implementing SIA is a plus that should not be overlooked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
